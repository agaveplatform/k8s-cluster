#jinja2:variable_start_string:'[%' , variable_end_string:'%]'

## Create default rules for monitoring the cluster
##
defaultRules:
  create: true

{% if prometheus_operator_prometheus_blackbox_targets|length > 0 %}
## Provide custom recording or alerting rules to be deployed into the cluster.
##
additionalPrometheusRules:
- name: blackbox-ssl-expiry.rules
  groups:
    - name: blackbox_ssl_expiry
      rules:
        - alert: SSLCertExpiringSoon
          expr: probe_ssl_earliest_cert_expiry{job="blackbox"} - time() < 86400 * 30
          for: 10m
    - name: blackbox_unreachable
      rules:
        - alert: SiteUnreachable
          expr: probe_success{job="blackbox"} < 1
          for: 10m
{% endif %}

##
global:
  rbac:
    create: true
    pspEnabled: true

## Configuration for alertmanager
## ref: https://prometheus.io/docs/alerting/alertmanager/
##
alertmanager:

  ## Deploy alertmanager
  ##
  enabled: true

  ## Alertmanager configuration directives
  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
  ##
  config:
    global:
      resolve_timeout: 5m
{#      slack_api_url: '[% prometheus_operator_slack_webhook_url %]'#}
    route:
      group_by: ['job']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'null'
      routes:
      - match:
          alertname: Watchdog
        receiver: 'null'
    receivers:
    - name: 'null'
{#    - name: 'default-receiver'#}
{#      slack_configs:#}
{#      - channel: '#api-test'#}
{#        title: "[% project_name %] - {{ {{ .GroupLabels.alertname }} - `{{ .Labels.severity }}`\n{{ end }}"#}
{#        text: "{{ range .Alerts }}{{ .Annotations.message }}\n{{ end }}"#}


  ## Settings affecting alertmanagerSpec
  ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#alertmanagerspec
  ##
  alertmanagerSpec:

    ## Log level for Alertmanager to be configured with.
    ##
    logLevel: debug

    ## Define Log Format
    # Use logfmt (default) or json-formatted logging
    logFormat: logfmt

    ## Size is the expected size of the alertmanager cluster. The controller will eventually make the size of the
    ## running cluster equal to the expected size.
    replicas: 1

    # Force the alertmanager instance to run on the first master
    # so we can pin the instance to local disk and avoid nfs
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/master
              operator: Exists
            - key: kubernetes.io/hostname
              operator: In
              values:
              - [% groups['kube-master'][0] %]

    ## Time duration Alertmanager shall retain data for. Default is '120h', and must match the regular expression
    ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).
    ##
    retention: [% prometheus_operator_alertmanager_retention %]

    ## Storage is the definition of how storage will be used by the Alertmanager instances.
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/storage.md
    ##
{% if prometheus_operator_alertmanager_storage_class != '' %}
    storage:
      volumeClaimTemplate:
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: [% prometheus_operator_alertmanager_pvc_size %]
          storageClassName: [% prometheus_operator_alertmanager_storage_class %]
{% else %}
    storage: {}
{% endif %}

## Using default values from https://github.com/helm/charts/blob/master/stable/grafana/values.yaml
##
grafana:
  enabled: true

  ## Deploy default dashboards.
  ##
  defaultDashboardsEnabled: true

  adminPassword: prom-operator

  ## Configure additional grafana datasources
  ## ref: http://docs.grafana.org/administration/provisioning/#datasources
  additionalDataSources:
  - name: Loki
    type: loki
    access: proxy
    url: http://loki.monitoring.svc:3100

  persistence:
{% if prometheus_operator_grafana_storage_class != '' %}
    enabled: true
    storageClassName: [% prometheus_operator_grafana_storage_class %]
    accessModes:
    - ReadWriteOnce
    size: [% prometheus_operator_grafana_pvc_size %]
{% else %}
    enabled: false
{% endif %}

  grafanaSpec:

    # Force the grafana instance to run on the first master
    # so we can pin the instance to local disk and avoid nfs
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/master
              operator: Exists
            - key: kubernetes.io/hostname
              operator: In
              values:
              - [% groups['kube-master'][0] %]


## Component scraping the kube api server
##
kubeApiServer:
  enabled: true
  tlsConfig:
    serverName: kubernetes
    insecureSkipVerify: false

  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""
    jobLabel: component
    selector:
      matchLabels:
        component: apiserver
        provider: kubernetes

## Component scraping the kubelet and kubelet-hosted cAdvisor
##
kubelet:
  enabled: true
  namespace: kube-system

  serviceMonitor:

    ## Enable scraping the kubelet over https. For requirements to enable this see
    ## https://github.com/coreos/prometheus-operator/issues/926
    ##
    https: false

    metricRelabelings:
      - action: replace
        sourceLabels:
          - node
        targetLabel: instance
    # - sourceLabels: [__name__, image]
    #   separator: ;
    #   regex: container_([a-z_]+);
    #   replacement: $1
    #   action: drop
    # - sourceLabels: [__name__]
    #   separator: ;
    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
    #   replacement: $1
    #   action: drop


## Component scraping the kube controller manager
##
kubeControllerManager:
  enabled: true

  ## If using kubeControllerManager.endpoints only the port and targetPort are used
  ##
  service:
    port: 10257
    targetPort: 10257
    selector:
      component: kube-controller-manager

  serviceMonitor:
    ## Enable scraping kube-controller-manager over https.
    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks
    ##
    https: true

    # Skip TLS certificate validation when scraping
    insecureSkipVerify: true


## Component scraping coreDns. Use either this or kubeDns
##
coreDns:
  enabled: true
  service:
    port: 9153
    targetPort: 9153
    selector:
      k8s-app: kube-dns


## Component scraping kubeDns. Use either this or coreDns
##
kubeDns:
  enabled: false


## Component scraping etcd
##
kubeEtcd:
  enabled: true

  ## If your etcd is not deployed as a pod, specify IPs it can be found on
  ##
  endpoints:
{% for etcd_host in groups['etcd'] %}
    - [% hostvars[etcd_host]['ip'] %]
{% endfor %}

  ## Configure secure access to the etcd cluster by loading a secret into prometheus and
  ## specifying security configuration below. For example, with a secret named etcd-client-cert
  ##
  serviceMonitor:
    scheme: https
    insecureSkipVerify: true
    serverName: null
    caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca
    certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client
    keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key


## Component scraping kube scheduler
##
kubeScheduler:
  enabled: true

  ## If using kubeScheduler.endpoints only the port and targetPort are used
  ##
  service:
    port: 10259
    targetPort: 10259
    selector:
      component: kube-scheduler

  serviceMonitor:
    ## Enable scraping kube-controller-manager over https.
    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks
    ##
    https: true

    ## Skip TLS certificate validation when scraping
    insecureSkipVerify: true


## Component scraping kube proxy
##
kubeProxy:
  service:
    port: 10249
    targetPort: 10249

## Deploy a Prometheus instance
##
prometheus:

  ## Settings affecting prometheusSpec
  ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
  ##
  prometheusSpec:

    # Force the prometheus instance to run on the first master
    # so we can pin the instance to local disk and avoid nfs
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/master
              operator: Exists
            - key: kubernetes.io/hostname
              operator: In
              values:
              - [% groups['kube-master'][0] %]

    ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
    ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not
    ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated
    ## with the new list of secrets.
    ##
    secrets:
      - etcd-client-cert # etcd client certs

    volumeMounts:

    ## Prometheus StorageSpec for persistent data
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/storage.md
    ##
{% if prometheus_operator_prometheus_storage_class != '' %}
    storageSpec:
      volumeClaimTemplate:
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: [% prometheus_operator_prometheus_pvc_size %]
          storageClassName: [% prometheus_operator_prometheus_storage_class %]
{% else %}
    storageSpec: {}
{% endif %}

    ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
    ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
    ## as specified in the official Prometheus documentation:
    ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#<scrape_config>. As scrape configs are
    ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
    ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
    ## scrape configs are going to break Prometheus after the upgrade.
    ##
    ## The scrape configuraiton example below will find master nodes, provided they have the name .*mst.*, relabel the
    ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
    ##
    additionalScrapeConfigs:
{% if ceph_pool_monitors is defined and ceph_pool_monitors|length > 0 %}
    - job_name: 'ceph'
      static_configs:
      - targets: [% ceph_pool_monitors | map('regex_replace', '(?P<host>.+):(?P<port>\\d+)', '\\g<host>:9283') | list | to_json %]
        labels:
          alias: ceph-exporter
{% endif %}
    - job_name: 'kubernetes-service'
      kubernetes_sd_configs:
      - role: service
      relabel_configs:
        # drop services created by KubeDB to avoid double counting.app_kubernetes_io_name="*.kubedb.com"
        - source_labels: [__meta_kubernetes_service_label_kubedb_com_role]
          separator: ;
          regex: stats
          action: drop
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_service_name]
          action: replace
          target_label: kubernetes_service_name
    - job_name: 'kubedb-databases'
      honor_labels: true
      honor_timestamps: false
      scheme: http
      scrape_interval: 20s
      scrape_timeout: 20s
      kubernetes_sd_configs:
      - role: service
      # by default Prometheus server select all kubernetes services as possible target.
      # relabel_configs is used to filter only desired endpoints
      relabel_configs:
      # only keep the stats services created by KubeDB for monitoring purpose which has "-stats" suffix
      - source_labels: [__meta_kubernetes_service_name]
        separator: ;
        regex: (.*-stats)
        action: keep
      # service created by KubeDB will have "kubedb.com/role". keep only those with "stats" role
      - source_labels: [__meta_kubernetes_service_label_kubedb_com_role]
        separator: ;
        regex: stats
        action: keep
      # service created by KubeDB will have "kubedb.com/kind" and "kubedb.com/name" annotations. keep only those services that have these annotations.
      - source_labels: [__meta_kubernetes_service_label_kubedb_com_kind, __meta_kubernetes_service_label_kubedb_com_name]
        separator: ;
        regex: (.*);(.*)
        action: keep
      # add service namespace as label to the scrapped metrics
      - source_labels: [__meta_kubernetes_namespace]
        separator: ;
        regex: (.*)
        target_label: kubernetes_namespace
        replacement: $1
        action: replace
      # add service name as a label to the scrapped metrics
      - source_labels: [__meta_kubernetes_service_name]
        separator: ;
        regex: (.*)
        target_label: kubernetes_service
        replacement: $1
        action: replace
      # add node as a label to the scrapped metrics
      - source_labels: [__meta_kubernetes_pod_node_name]
        separator: ;
        regex: (.*)
        target_label: kubernetes_host
        replacement: $1
        action: replace
      # add stats service's labels to the scrapped metrics
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
{% if prometheus_operator_prometheus_blackbox_targets is defined and prometheus_operator_prometheus_blackbox_targets|length > 0 %}
    - job_name: 'blackbox'
      metrics_path: /probe
      params:
        module: [http_2xx,]
      static_configs:
      - targets:
{% for static_site_url in prometheus_operator_prometheus_blackbox_targets %}
        - [% static_site_url %]
{% endfor %}
      relabel_configs:
      - source_labels: [__address__]
        regex: (.*)(:80)?
        target_label: __param_target
      - source_labels: [__param_target]
        regex: (.*)
        target_label: instance
        replacement: ${1}
      - source_labels: []
        regex: .*
        target_label: __address__
        replacement: prometheus-blackbox-exporter:9115
{% endif %}
